{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bellopasal/opt/anaconda3/envs/currito/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import modes.mode_2 as model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/Users/bellopasal/PycharmProjects/simulador_1/currito_raspb/bus.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from picamera2 import Picamera2\n",
    "import RPi.GPIO as GPIO\n",
    "import time\n",
    "\n",
    "# Initialize the Raspberry Pi GPIO\n",
    "GPIO.setmode(GPIO.BCM)  # Use BCM numbering\n",
    "SIGNAL_PIN = 17         # GPIO pin to listen for a signal\n",
    "GPIO.setup(SIGNAL_PIN, GPIO.IN, pull_up_down=GPIO.PUD_DOWN)  # Configure as input with pull-down\n",
    "\n",
    "# Initialize Picamera2\n",
    "picam = Picamera2()\n",
    "picam.configure(picam.create_still_configuration())\n",
    "\n",
    "def capture_picture():\n",
    "    \"\"\"Captures a picture and saves it with a timestamp.\"\"\"\n",
    "    timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = f\"image_{timestamp}.jpg\"\n",
    "    picam.start()\n",
    "    time.sleep(2)  # Allow the camera to adjust (optional, depends on your setup)\n",
    "    picam.capture_file(filename)\n",
    "    picam.stop()\n",
    "    print(f\"Captured {filename}\")\n",
    "\n",
    "try:\n",
    "    print(\"Waiting for signal...\")\n",
    "    while True:\n",
    "        # Wait for the signal pin to go HIGH\n",
    "        if GPIO.input(SIGNAL_PIN) == GPIO.HIGH:\n",
    "            print(\"Signal detected!\")\n",
    "            capture_picture()\n",
    "            time.sleep(1)  # Avoid capturing multiple images for a single signal\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Exiting program...\")\n",
    "\n",
    "finally:\n",
    "    GPIO.cleanup()  # Clean up GPIO state when exiting\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.38 ðŸš€ Python-3.10.15 torch-2.2.2 CPU (Apple M1 Pro)\n",
      "YOLO11n summary (fused): 238 layers, 2,616,248 parameters, 0 gradients, 6.5 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W NNPACK.cpp:64] Could not initialize NNPACK! Reason: Unsupported hardware.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'yolo11n.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 84, 8400) (5.4 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mTorchScript:\u001b[0m starting export with torch 2.2.2...\n",
      "\u001b[34m\u001b[1mTorchScript:\u001b[0m export success âœ… 2.2s, saved as 'yolo11n.torchscript' (10.5 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mNCNN:\u001b[0m starting export with NCNN 1.0.20240820...\n",
      "\u001b[34m\u001b[1mNCNN:\u001b[0m running '/Users/bellopasal/opt/anaconda3/envs/currito/lib/python3.10/site-packages/ultralytics/pnnx yolo11n.torchscript ncnnparam=yolo11n_ncnn_model/model.ncnn.param ncnnbin=yolo11n_ncnn_model/model.ncnn.bin ncnnpy=yolo11n_ncnn_model/model_ncnn.py pnnxparam=yolo11n_ncnn_model/model.pnnx.param pnnxbin=yolo11n_ncnn_model/model.pnnx.bin pnnxpy=yolo11n_ncnn_model/model_pnnx.py pnnxonnx=yolo11n_ncnn_model/model.pnnx.onnx fp16=0 device=cpu inputshape=\"[1, 3, 640, 640]\"'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pnnxparam = yolo11n_ncnn_model/model.pnnx.param\n",
      "pnnxbin = yolo11n_ncnn_model/model.pnnx.bin\n",
      "pnnxpy = yolo11n_ncnn_model/model_pnnx.py\n",
      "pnnxonnx = yolo11n_ncnn_model/model.pnnx.onnx\n",
      "ncnnparam = yolo11n_ncnn_model/model.ncnn.param\n",
      "ncnnbin = yolo11n_ncnn_model/model.ncnn.bin\n",
      "ncnnpy = yolo11n_ncnn_model/model_ncnn.py\n",
      "fp16 = 0\n",
      "optlevel = 2\n",
      "device = cpu\n",
      "inputshape = [1,3,640,640]f32\n",
      "inputshape2 = \n",
      "customop = \n",
      "moduleop = \n",
      "############# pass_level0\n",
      "inline module = torch.nn.modules.linear.Identity\n",
      "inline module = ultralytics.nn.modules.block.Attention\n",
      "inline module = ultralytics.nn.modules.block.Bottleneck\n",
      "inline module = ultralytics.nn.modules.block.C2PSA\n",
      "inline module = ultralytics.nn.modules.block.C3k\n",
      "inline module = ultralytics.nn.modules.block.C3k2\n",
      "inline module = ultralytics.nn.modules.block.DFL\n",
      "inline module = ultralytics.nn.modules.block.PSABlock\n",
      "inline module = ultralytics.nn.modules.block.SPPF\n",
      "inline module = ultralytics.nn.modules.conv.Concat\n",
      "inline module = ultralytics.nn.modules.conv.Conv\n",
      "inline module = ultralytics.nn.modules.conv.DWConv\n",
      "inline module = ultralytics.nn.modules.head.Detect\n",
      "inline module = torch.nn.modules.linear.Identity\n",
      "inline module = ultralytics.nn.modules.block.Attention\n",
      "inline module = ultralytics.nn.modules.block.Bottleneck\n",
      "inline module = ultralytics.nn.modules.block.C2PSA\n",
      "inline module = ultralytics.nn.modules.block.C3k\n",
      "inline module = ultralytics.nn.modules.block.C3k2\n",
      "inline module = ultralytics.nn.modules.block.DFL\n",
      "inline module = ultralytics.nn.modules.block.PSABlock\n",
      "inline module = ultralytics.nn.modules.block.SPPF\n",
      "inline module = ultralytics.nn.modules.conv.Concat\n",
      "inline module = ultralytics.nn.modules.conv.Conv\n",
      "inline module = ultralytics.nn.modules.conv.DWConv\n",
      "inline module = ultralytics.nn.modules.head.Detect\n",
      "\n",
      "----------------\n",
      "\n",
      "[W1127 20:15:20.260203000 NNPACK.cpp:61] Could not initialize NNPACK! Reason: Unsupported hardware.\n",
      "############# pass_level1\n",
      "############# pass_level2\n",
      "############# pass_level3\n",
      "############# pass_level4\n",
      "############# pass_level5\n",
      "############# pass_ncnn\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mNCNN:\u001b[0m export success âœ… 1.7s, saved as 'yolo11n_ncnn_model' (10.2 MB)\n",
      "\n",
      "Export complete (4.4s)\n",
      "Results saved to \u001b[1m/Users/bellopasal/PycharmProjects/simulador_1/currito_raspb\u001b[0m\n",
      "Predict:         yolo predict task=detect model=yolo11n_ncnn_model imgsz=640  \n",
      "Validate:        yolo val task=detect model=yolo11n_ncnn_model imgsz=640 data=/usr/src/ultralytics/ultralytics/cfg/datasets/coco.yaml  \n",
      "Visualize:       https://netron.app\n",
      "WARNING âš ï¸ Unable to automatically guess model task, assuming 'task=detect'. Explicitly define task for your model, i.e. 'task=detect', 'segment', 'classify','pose' or 'obb'.\n",
      "Loading yolo11n_ncnn_model for NCNN inference...\n",
      "\n",
      "image 1/1 /Users/bellopasal/PycharmProjects/simulador_1/currito_raspb/images.jpg: 640x640 1 orange, 118.0ms\n",
      "Speed: 3.8ms preprocess, 118.0ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Largest prediction: {'class': 'orange', 'confidence': tensor([0.4993]), 'bbox': [113.94820404052734, 119.28294372558594, 178.6775360107422, 171.11123657226562]}\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load a YOLO11n PyTorch model\n",
    "model = YOLO(\"yolo11n.pt\")\n",
    "\n",
    "# Export the model to NCNN format\n",
    "model.export(format=\"ncnn\")  # creates 'yolo11n_ncnn_model'\n",
    "\n",
    "# Load the exported NCNN model\n",
    "ncnn_model = YOLO(\"yolo11n_ncnn_model\")\n",
    "\n",
    "largest_prediction = None\n",
    "largest_area = 0\n",
    "image_path = \"images.jpg\"\n",
    "\n",
    "results = ncnn_model(image_path)\n",
    "\n",
    "\n",
    "largest_prediction = None\n",
    "largest_area = 0\n",
    "\n",
    "for result in results:\n",
    "    # Access detected bounding boxes\n",
    "    for detection in result.boxes:\n",
    "        # Print the entire detection object to inspect its contents\n",
    "        #print(f\"Detection: {detection}\")\n",
    "        \n",
    "        # Check if detection.xywh exists and is in the expected format\n",
    "        if hasattr(detection, 'xywh'):\n",
    "            bbox = detection.xywh.tolist() \n",
    "            bbox = [item for sublist in bbox for item in sublist] # Convert bbox to a list\n",
    "\n",
    "            # Ensure bbox has at least 4 elements: [x, y, width, height]\n",
    "            if len(bbox) >= 4:\n",
    "                width, height = bbox[2], bbox[3]\n",
    "                area = width * height\n",
    "\n",
    "                # If this is the largest box, store its prediction\n",
    "                if area > largest_area:\n",
    "                    largest_area = area\n",
    "                    confidence = detection.conf\n",
    "                    class_index = detection.cls.item()  # Convert tensor to scalar\n",
    "                    class_name = result.names[class_index]  # Use scalar index to get the class name\n",
    "                    \n",
    "                    largest_prediction = {\n",
    "                        \"class\": class_name,\n",
    "                        \"confidence\": confidence,\n",
    "                        \"bbox\": bbox\n",
    "                    }\n",
    "            else:\n",
    "                print(f\"Invalid bbox format (less than 4 elements): {bbox}\")\n",
    "        else:\n",
    "            print(\"detection.xywh is missing or not in the expected format.\")\n",
    "    \n",
    "# Print the largest prediction\n",
    "if largest_prediction:\n",
    "    print(\"Largest prediction:\", largest_prediction)\n",
    "else:\n",
    "    print(\"No valid bounding boxes found.\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"1. **Oranges are a hybrid:**  The sweet orange we commonly eat isn't a naturally occurring species. It's believed to be a hybrid of the pomelo and mandarin orange, likely originating in Southeast Asia.\\n\\n2. **Orange peel has surprising uses:**  Beyond just zest for flavoring, orange peel contains essential oils used in perfumes and cleaning products.  It's also rich in pectin, a gelling agent used in jams and jellies, and has been explored for its potential health benefits due to its antioxidant properties.\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.say_something(largest_prediction[\"class\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: None\n",
       " masks: None\n",
       " names: {0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 4: 'airplane', 5: 'bus', 6: 'train', 7: 'truck', 8: 'boat', 9: 'traffic light', 10: 'fire hydrant', 11: 'stop sign', 12: 'parking meter', 13: 'bench', 14: 'bird', 15: 'cat', 16: 'dog', 17: 'horse', 18: 'sheep', 19: 'cow', 20: 'elephant', 21: 'bear', 22: 'zebra', 23: 'giraffe', 24: 'backpack', 25: 'umbrella', 26: 'handbag', 27: 'tie', 28: 'suitcase', 29: 'frisbee', 30: 'skis', 31: 'snowboard', 32: 'sports ball', 33: 'kite', 34: 'baseball bat', 35: 'baseball glove', 36: 'skateboard', 37: 'surfboard', 38: 'tennis racket', 39: 'bottle', 40: 'wine glass', 41: 'cup', 42: 'fork', 43: 'knife', 44: 'spoon', 45: 'bowl', 46: 'banana', 47: 'apple', 48: 'sandwich', 49: 'orange', 50: 'broccoli', 51: 'carrot', 52: 'hot dog', 53: 'pizza', 54: 'donut', 55: 'cake', 56: 'chair', 57: 'couch', 58: 'potted plant', 59: 'bed', 60: 'dining table', 61: 'toilet', 62: 'tv', 63: 'laptop', 64: 'mouse', 65: 'remote', 66: 'keyboard', 67: 'cell phone', 68: 'microwave', 69: 'oven', 70: 'toaster', 71: 'sink', 72: 'refrigerator', 73: 'book', 74: 'clock', 75: 'vase', 76: 'scissors', 77: 'teddy bear', 78: 'hair drier', 79: 'toothbrush'}\n",
       " obb: None\n",
       " orig_img: array([[[119, 146, 172],\n",
       "         [121, 148, 174],\n",
       "         [122, 152, 177],\n",
       "         ...,\n",
       "         [161, 171, 188],\n",
       "         [160, 170, 187],\n",
       "         [160, 170, 187]],\n",
       " \n",
       "        [[120, 147, 173],\n",
       "         [122, 149, 175],\n",
       "         [123, 153, 178],\n",
       "         ...,\n",
       "         [161, 171, 188],\n",
       "         [160, 170, 187],\n",
       "         [160, 170, 187]],\n",
       " \n",
       "        [[123, 150, 176],\n",
       "         [124, 151, 177],\n",
       "         [125, 155, 180],\n",
       "         ...,\n",
       "         [161, 171, 188],\n",
       "         [160, 170, 187],\n",
       "         [160, 170, 187]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[183, 182, 186],\n",
       "         [179, 178, 182],\n",
       "         [180, 179, 183],\n",
       "         ...,\n",
       "         [121, 111, 117],\n",
       "         [113, 103, 109],\n",
       "         [115, 105, 111]],\n",
       " \n",
       "        [[165, 164, 168],\n",
       "         [173, 172, 176],\n",
       "         [187, 186, 190],\n",
       "         ...,\n",
       "         [102,  92,  98],\n",
       "         [101,  91,  97],\n",
       "         [103,  93,  99]],\n",
       " \n",
       "        [[123, 122, 126],\n",
       "         [145, 144, 148],\n",
       "         [176, 175, 179],\n",
       "         ...,\n",
       "         [ 95,  85,  91],\n",
       "         [ 96,  86,  92],\n",
       "         [ 98,  88,  94]]], dtype=uint8)\n",
       " orig_shape: (1080, 810)\n",
       " path: '/Users/bellopasal/PycharmProjects/simulador_1/currito_raspb/bus.jpg'\n",
       " probs: None\n",
       " save_dir: 'runs/detect/predict'\n",
       " speed: {'preprocess': 6.317138671875, 'inference': 92.63205528259277, 'postprocess': 3.968954086303711}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bellopasal/opt/anaconda3/envs/currito/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Los Ãºltimos presidentes del Gobierno de EspaÃ±a han sido:\n",
      "\n",
      "* **Pedro SÃ¡nchez PÃ©rez-CastejÃ³n** (desde el 2 de junio de 2018)\n",
      "* **Mariano Rajoy Brey** (desde el 21 de diciembre de 2011 hasta el 1 de junio de 2018)\n",
      "* **JosÃ© Luis RodrÃ­guez Zapatero** (desde el 17 de abril de 2004 hasta el 21 de diciembre de 2011)\n",
      "* **JosÃ© MarÃ­a Aznar LÃ³pez** (desde el 5 de mayo de 1996 hasta el 17 de abril de 2004)\n",
      "* **Felipe GonzÃ¡lez MÃ¡rquez** (desde el 5 de diciembre de 1982 hasta el 5 de mayo de 1996)\n",
      "\n",
      "\n",
      "Cabe destacar que antes de la TransiciÃ³n EspaÃ±ola, existÃ­an Jefes de Estado (como el General Franco) y Presidentes del Gobierno con roles y sistemas diferentes al actual.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import modes.mode_2 as mode_2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/bellopasal/.cache/torch/hub/ultralytics_yolov5_master\n",
      "YOLOv5 ðŸš€ 2024-11-27 Python-3.10.15 torch-2.2.2 CPU\n",
      "\n",
      "Fusing layers... \n",
      "[W NNPACK.cpp:64] Could not initialize NNPACK! Reason: Unsupported hardware.\n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        ...,\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255]],\n",
       "\n",
       "       [[255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        ...,\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255]],\n",
       "\n",
       "       [[255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        ...,\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        ...,\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255]],\n",
       "\n",
       "       [[255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        ...,\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255]],\n",
       "\n",
       "       [[255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        ...,\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255],\n",
       "        [255, 255, 255]]], dtype=uint8)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o = mode_2.ObjectDetector()\n",
    "\n",
    "o.detect_objects_in_image(image_path=\"images.jpeg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2654069080.py, line 78)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[19], line 78\u001b[0;36m\u001b[0m\n\u001b[0;31m    if (cv2.waitKey(1) & 0xFF == ord('q')) or :\u001b[0m\n\u001b[0m                                              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "\n",
    "class RealTimeObjectDetector:\n",
    "    def __init__(self, model_path, area_threshold):\n",
    "        \"\"\"\n",
    "        Initialize the detector with a YOLO model and area threshold.\n",
    "        \"\"\"\n",
    "        self.model = YOLO(model_path)\n",
    "        self.area_threshold = area_threshold\n",
    "\n",
    "    def _process_frame(self, frame):\n",
    "        \"\"\"\n",
    "        Detect objects in a single frame and return the largest prediction\n",
    "        that exceeds the area threshold.\n",
    "        \"\"\"\n",
    "        results = self.model(frame)\n",
    "        largest_prediction = None\n",
    "        largest_area = 0\n",
    "\n",
    "        for result in results:\n",
    "            for detection in result.boxes:\n",
    "                if hasattr(detection, 'xywh'):\n",
    "                    bbox = detection.xywh.tolist()\n",
    "                    bbox = [item for sublist in bbox for item in sublist]  # Flatten list\n",
    "\n",
    "                    if len(bbox) >= 4:\n",
    "                        width, height = bbox[2], bbox[3]\n",
    "                        area = width * height\n",
    "\n",
    "                        if area > self.area_threshold and area > largest_area:\n",
    "                            largest_area = area\n",
    "                            confidence = detection.conf\n",
    "                            class_index = detection.cls.item()  # Convert tensor to scalar\n",
    "                            class_name = result.names[class_index]  # Map index to class name\n",
    "\n",
    "                            largest_prediction = {\n",
    "                                \"class\": class_name,\n",
    "                                \"confidence\": confidence,\n",
    "                                \"bbox\": bbox,\n",
    "                            }\n",
    "        return largest_prediction\n",
    "\n",
    "    def start_real_time_detection(self):\n",
    "        \"\"\"\n",
    "        Start real-time object detection from the webcam.\n",
    "        \"\"\"\n",
    "        cap = cv2.VideoCapture(0)  # Use 0 for default webcam; change for other video sources\n",
    "\n",
    "        if not cap.isOpened():\n",
    "            print(\"Error: Unable to access the webcam.\")\n",
    "            return\n",
    "\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                print(\"Error: Unable to read a frame from the webcam.\")\n",
    "                break\n",
    "\n",
    "            # Process the current frame\n",
    "            prediction = self._process_frame(frame)\n",
    "\n",
    "            # Annotate the frame if a valid prediction exists\n",
    "            if prediction:\n",
    "                x, y, w, h = prediction[\"bbox\"]\n",
    "                # cv2.rectangle(frame, (int(x - w / 2), int(y - h / 2)),\n",
    "                #               (int(x + w / 2), int(y + h / 2)), (0, 255, 0), 2)\n",
    "                # cv2.putText(frame, f\"{prediction['class']} ({prediction['confidence']:.2f})\",\n",
    "                #             (int(x - w / 2), int(y - h / 2) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "                \n",
    "                print(\"Prediction with sufficient area found:\", prediction)\n",
    "                return prediction['class']\n",
    "                break\n",
    "\n",
    "\n",
    "            # Exit on 'q' key press\n",
    "            if (cv2.waitKey(1) & 0xFF == ord('q')) :\n",
    "                break\n",
    "\n",
    "        cap.release()\n",
    "        \n",
    "\n",
    "\n",
    "real_time_detector = RealTimeObjectDetector(\"yolo11n_ncnn_model\", area_threshold=5000)  # Adjust threshold as needed\n",
    "real_time_detector.start_real_time_detection()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "currito",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
